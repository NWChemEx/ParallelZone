<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>parallelzone: parallelzone::mpi_helpers::detail_::CommPPPIMPL Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">parallelzone
   &#160;<span id="projectnumber">0.1.32</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><b>parallelzone</b></li><li class="navelem"><b>mpi_helpers</b></li><li class="navelem"><b>detail_</b></li><li class="navelem"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html">CommPPPIMPL</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-types">Public Types</a> &#124;
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">parallelzone::mpi_helpers::detail_::CommPPPIMPL Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>Basic implementations and state for the <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html" title="A little wrapper around an MPI communicator to make it more C++ friendly.">CommPP</a> class.  
 <a href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="commpp__pimpl_8hpp_source.html">commpp_pimpl.hpp</a>&gt;</code></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-types"></a>
Public Types</h2></td></tr>
<tr class="memitem:a37aada0957a203b7cf108b8bddadd8e3"><td class="memItemLeft" align="right" valign="top"><a id="a37aada0957a203b7cf108b8bddadd8e3"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a37aada0957a203b7cf108b8bddadd8e3">parent_type</a> = <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html">CommPP</a></td></tr>
<tr class="memdesc:a37aada0957a203b7cf108b8bddadd8e3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type of the class we are implementing. <br /></td></tr>
<tr class="separator:a37aada0957a203b7cf108b8bddadd8e3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab2087ccfbcb99fced6b2f12565e91f53"><td class="memItemLeft" align="right" valign="top"><a id="ab2087ccfbcb99fced6b2f12565e91f53"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#ab2087ccfbcb99fced6b2f12565e91f53">pimpl_pointer</a> = <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#acee768cd31f3c6592033c8c4b9ef1511">parent_type::pimpl_pointer</a></td></tr>
<tr class="memdesc:ab2087ccfbcb99fced6b2f12565e91f53"><td class="mdescLeft">&#160;</td><td class="mdescRight">Ultimately a typedef of <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#acee768cd31f3c6592033c8c4b9ef1511" title="Type of a pointer to the PIMPL.">CommPP::pimpl_pointer</a>. <br /></td></tr>
<tr class="separator:ab2087ccfbcb99fced6b2f12565e91f53"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae38fa19918285749c10ac006cb982829"><td class="memItemLeft" align="right" valign="top"><a id="ae38fa19918285749c10ac006cb982829"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#ae38fa19918285749c10ac006cb982829">mpi_comm_type</a> = <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#ac3d74cca17008c02706ab32ce0ed8374">parent_type::mpi_comm_type</a></td></tr>
<tr class="memdesc:ae38fa19918285749c10ac006cb982829"><td class="mdescLeft">&#160;</td><td class="mdescRight">Ultimately a typedef of <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#ac3d74cca17008c02706ab32ce0ed8374" title="Type of a bare MPI communicator.">CommPP::mpi_comm_type</a>. <br /></td></tr>
<tr class="separator:ae38fa19918285749c10ac006cb982829"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1126b67f0c397db003259bc0006f494e"><td class="memItemLeft" align="right" valign="top"><a id="a1126b67f0c397db003259bc0006f494e"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a1126b67f0c397db003259bc0006f494e">size_type</a> = <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#a20854b131da254b8288d0fbd0b64be83">parent_type::size_type</a></td></tr>
<tr class="memdesc:a1126b67f0c397db003259bc0006f494e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Ultimately a typedef of <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#a20854b131da254b8288d0fbd0b64be83" title="Type used for indexing and offsets.">CommPP::size_type</a>. <br /></td></tr>
<tr class="separator:a1126b67f0c397db003259bc0006f494e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac8a3363be5362383942885b32bc90659"><td class="memItemLeft" align="right" valign="top"><a id="ac8a3363be5362383942885b32bc90659"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#ac8a3363be5362383942885b32bc90659">binary_type</a> = <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#a31ed5fa6980dcdb5811d08fe3bfb9692">parent_type::binary_type</a></td></tr>
<tr class="memdesc:ac8a3363be5362383942885b32bc90659"><td class="mdescLeft">&#160;</td><td class="mdescRight">Ultimately a typedef of <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#a31ed5fa6980dcdb5811d08fe3bfb9692" title="Type of a block of binary data.">CommPP::binary_type</a>. <br /></td></tr>
<tr class="separator:ac8a3363be5362383942885b32bc90659"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aac4431ffb4dff217555cf3313b50106b"><td class="memItemLeft" align="right" valign="top"><a id="aac4431ffb4dff217555cf3313b50106b"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#aac4431ffb4dff217555cf3313b50106b">binary_reference</a> = <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#a2a03ce7e6ca24f81fc97c87f83c986b2">parent_type::binary_reference</a></td></tr>
<tr class="memdesc:aac4431ffb4dff217555cf3313b50106b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Ultimately a typedef of <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#a2a03ce7e6ca24f81fc97c87f83c986b2" title="Type of a read/write reference to a block of binary data.">CommPP::binary_reference</a>. <br /></td></tr>
<tr class="separator:aac4431ffb4dff217555cf3313b50106b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6c066ef36dc8d95481e11b67d18fae6d"><td class="memItemLeft" align="right" valign="top"><a id="a6c066ef36dc8d95481e11b67d18fae6d"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a6c066ef36dc8d95481e11b67d18fae6d">const_binary_reference</a> = <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#af22722fe3a989dd052d37cacab18ccbc">parent_type::const_binary_reference</a></td></tr>
<tr class="memdesc:a6c066ef36dc8d95481e11b67d18fae6d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Ultiamtely a typedef of <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#af22722fe3a989dd052d37cacab18ccbc" title="Type of a read-only reference to a block of binary data.">CommPP::const_binary_reference</a>. <br /></td></tr>
<tr class="separator:a6c066ef36dc8d95481e11b67d18fae6d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5dfa893cb94c40a93ad0f31b31169112"><td class="memItemLeft" align="right" valign="top"><a id="a5dfa893cb94c40a93ad0f31b31169112"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a5dfa893cb94c40a93ad0f31b31169112">binary_gather_return</a> = <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#ab4596253d9e035175f9c5d05794f6580">parent_type::binary_gather_return</a></td></tr>
<tr class="memdesc:a5dfa893cb94c40a93ad0f31b31169112"><td class="mdescLeft">&#160;</td><td class="mdescRight">Ultimately a typedef of <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#ab4596253d9e035175f9c5d05794f6580" title="Type returned by the binary version of gather.">CommPP::binary_gather_return</a>. <br /></td></tr>
<tr class="separator:a5dfa893cb94c40a93ad0f31b31169112"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af7e39163f05a7e6dc94fdbf496d23354"><td class="memItemLeft" align="right" valign="top"><a id="af7e39163f05a7e6dc94fdbf496d23354"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#af7e39163f05a7e6dc94fdbf496d23354">gatherv_pair</a> = <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#aba97438bea80c9caa7d60688d7f313fa">parent_type::gatherv_pair</a></td></tr>
<tr class="memdesc:af7e39163f05a7e6dc94fdbf496d23354"><td class="mdescLeft">&#160;</td><td class="mdescRight">Ultimately a typedef of <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#aba97438bea80c9caa7d60688d7f313fa" title="Type of a buffer and the sizes per rank.">CommPP::gatherv_pair</a>. <br /></td></tr>
<tr class="separator:af7e39163f05a7e6dc94fdbf496d23354"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a99f472c9d63bb84fb2c21d7df17164b4"><td class="memItemLeft" align="right" valign="top"><a id="a99f472c9d63bb84fb2c21d7df17164b4"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a99f472c9d63bb84fb2c21d7df17164b4">binary_gatherv_return</a> = <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#a8ace65a322c1401ddaa5219c8a18b2fd">parent_type::binary_gatherv_return</a></td></tr>
<tr class="memdesc:a99f472c9d63bb84fb2c21d7df17164b4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Ultimately a typedef of <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html#a8ace65a322c1401ddaa5219c8a18b2fd" title="Type returned by the binary version of gatherv.">CommPP::binary_gatherv_return</a>. <br /></td></tr>
<tr class="separator:a99f472c9d63bb84fb2c21d7df17164b4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af2364d851859f49c08b553ac68ea7aa2"><td class="memItemLeft" align="right" valign="top"><a id="af2364d851859f49c08b553ac68ea7aa2"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#af2364d851859f49c08b553ac68ea7aa2">opt_root_t</a> = std::optional&lt; <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a1126b67f0c397db003259bc0006f494e">size_type</a> &gt;</td></tr>
<tr class="memdesc:af2364d851859f49c08b553ac68ea7aa2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type of an optional root. <br /></td></tr>
<tr class="separator:af2364d851859f49c08b553ac68ea7aa2"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:aed707ecc7ace9b6829033d3cdec85eef"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#aed707ecc7ace9b6829033d3cdec85eef">CommPPPIMPL</a> (<a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#ae38fa19918285749c10ac006cb982829">mpi_comm_type</a> <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a338e9de5958ab34e16999b9bd966cf32">comm</a>)</td></tr>
<tr class="memdesc:aed707ecc7ace9b6829033d3cdec85eef"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initializes *this from the MPI communicator <code>comm</code>.  <a href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#aed707ecc7ace9b6829033d3cdec85eef">More...</a><br /></td></tr>
<tr class="separator:aed707ecc7ace9b6829033d3cdec85eef"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a91dc88dc6ac72f0c7ace2f674cf3428d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#ab2087ccfbcb99fced6b2f12565e91f53">pimpl_pointer</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a91dc88dc6ac72f0c7ace2f674cf3428d">clone</a> () const</td></tr>
<tr class="memdesc:a91dc88dc6ac72f0c7ace2f674cf3428d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Makes a deep copy of *this.  <a href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a91dc88dc6ac72f0c7ace2f674cf3428d">More...</a><br /></td></tr>
<tr class="separator:a91dc88dc6ac72f0c7ace2f674cf3428d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a338e9de5958ab34e16999b9bd966cf32"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#ae38fa19918285749c10ac006cb982829">mpi_comm_type</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a338e9de5958ab34e16999b9bd966cf32">comm</a> () const noexcept</td></tr>
<tr class="memdesc:a338e9de5958ab34e16999b9bd966cf32"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the MPI communicator behind *this.  <a href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a338e9de5958ab34e16999b9bd966cf32">More...</a><br /></td></tr>
<tr class="separator:a338e9de5958ab34e16999b9bd966cf32"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeb6195c8087033113ce35d2eb9304597"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a1126b67f0c397db003259bc0006f494e">size_type</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#aeb6195c8087033113ce35d2eb9304597">size</a> () const noexcept</td></tr>
<tr class="memdesc:aeb6195c8087033113ce35d2eb9304597"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the number of ranks manged by <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a338e9de5958ab34e16999b9bd966cf32" title="Returns the MPI communicator behind *this.">comm()</a>  <a href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#aeb6195c8087033113ce35d2eb9304597">More...</a><br /></td></tr>
<tr class="separator:aeb6195c8087033113ce35d2eb9304597"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a10dff15a59650bbd04067d1bddd57929"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a1126b67f0c397db003259bc0006f494e">size_type</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a10dff15a59650bbd04067d1bddd57929">me</a> () const noexcept</td></tr>
<tr class="memdesc:a10dff15a59650bbd04067d1bddd57929"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the rank of the current process.  <a href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a10dff15a59650bbd04067d1bddd57929">More...</a><br /></td></tr>
<tr class="separator:a10dff15a59650bbd04067d1bddd57929"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a031d47e35fc8fa28bcb5795a161218b6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a5dfa893cb94c40a93ad0f31b31169112">binary_gather_return</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a031d47e35fc8fa28bcb5795a161218b6">gather</a> (<a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a6c066ef36dc8d95481e11b67d18fae6d">const_binary_reference</a> data, <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#af2364d851859f49c08b553ac68ea7aa2">opt_root_t</a> root=std::nullopt) const</td></tr>
<tr class="memdesc:a031d47e35fc8fa28bcb5795a161218b6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Binary-based gather call which creates a buffer for the result.  <a href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a031d47e35fc8fa28bcb5795a161218b6">More...</a><br /></td></tr>
<tr class="separator:a031d47e35fc8fa28bcb5795a161218b6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aadd9a3592211f25ca1c4ff08abfe0b88"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#aadd9a3592211f25ca1c4ff08abfe0b88">gather</a> (<a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a6c066ef36dc8d95481e11b67d18fae6d">const_binary_reference</a> data, <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#aac4431ffb4dff217555cf3313b50106b">binary_reference</a> out_buffer, <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#af2364d851859f49c08b553ac68ea7aa2">opt_root_t</a> root=std::nullopt) const</td></tr>
<tr class="memdesc:aadd9a3592211f25ca1c4ff08abfe0b88"><td class="mdescLeft">&#160;</td><td class="mdescRight">Binary-based gather call that can avoid copies.  <a href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#aadd9a3592211f25ca1c4ff08abfe0b88">More...</a><br /></td></tr>
<tr class="separator:aadd9a3592211f25ca1c4ff08abfe0b88"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18f149e5acccd0d544ddd6257c6a9170"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a99f472c9d63bb84fb2c21d7df17164b4">binary_gatherv_return</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a18f149e5acccd0d544ddd6257c6a9170">gatherv</a> (<a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a6c066ef36dc8d95481e11b67d18fae6d">const_binary_reference</a> data, <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#af2364d851859f49c08b553ac68ea7aa2">opt_root_t</a> root=std::nullopt) const</td></tr>
<tr class="memdesc:a18f149e5acccd0d544ddd6257c6a9170"><td class="mdescLeft">&#160;</td><td class="mdescRight">Analog of gather(data, root) where the length of data can vary.  <a href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a18f149e5acccd0d544ddd6257c6a9170">More...</a><br /></td></tr>
<tr class="separator:a18f149e5acccd0d544ddd6257c6a9170"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac75b2fe878e602110638bf5d73b0b5ca"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#ac75b2fe878e602110638bf5d73b0b5ca">operator==</a> (const <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html">CommPPPIMPL</a> &amp;rhs) const noexcept</td></tr>
<tr class="memdesc:ac75b2fe878e602110638bf5d73b0b5ca"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compares *this to another PIMPL instance.  <a href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#ac75b2fe878e602110638bf5d73b0b5ca">More...</a><br /></td></tr>
<tr class="separator:ac75b2fe878e602110638bf5d73b0b5ca"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Basic implementations and state for the <a class="el" href="classparallelzone_1_1mpi__helpers_1_1CommPP.html" title="A little wrapper around an MPI communicator to make it more C++ friendly.">CommPP</a> class. </p>
<p>This class primarily exists to facilitate unit testing the implementations of the MPI ops without exposing them through the public API. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="aed707ecc7ace9b6829033d3cdec85eef"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aed707ecc7ace9b6829033d3cdec85eef">&#9670;&nbsp;</a></span>CommPPPIMPL()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">parallelzone::mpi_helpers::detail_::CommPPPIMPL::CommPPPIMPL </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#ae38fa19918285749c10ac006cb982829">mpi_comm_type</a>&#160;</td>
          <td class="paramname"><em>comm</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">explicit</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Initializes *this from the MPI communicator <code>comm</code>. </p>
<p>This ctor inspects <code>comm</code> and determines:</p>
<ul>
<li>The current MPI rank via MPI_Comm_rank</li>
<li>The current MPI comm size via MPI_Comm_size</li>
</ul>
<p>the results are cached in *this.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">comm</td><td>Handle to the MPI communicator used for initialization. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a91dc88dc6ac72f0c7ace2f674cf3428d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a91dc88dc6ac72f0c7ace2f674cf3428d">&#9670;&nbsp;</a></span>clone()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#ab2087ccfbcb99fced6b2f12565e91f53">pimpl_pointer</a> parallelzone::mpi_helpers::detail_::CommPPPIMPL::clone </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Makes a deep copy of *this. </p>
<p>This method creates a copy of *this by deep copying the wrapped MPI communicator handle, the rank of the current process, and the size of the communicator.</p>
<dl class="section return"><dt>Returns</dt><dd>A deep copy of *this on the heap. </dd></dl>

</div>
</div>
<a id="a338e9de5958ab34e16999b9bd966cf32"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a338e9de5958ab34e16999b9bd966cf32">&#9670;&nbsp;</a></span>comm()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#ae38fa19918285749c10ac006cb982829">mpi_comm_type</a> parallelzone::mpi_helpers::detail_::CommPPPIMPL::comm </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the MPI communicator behind *this. </p>
<p>When *this was constructed it was given the handle to an MPI communicator. This method can be used to retrieve that handle.</p>
<dl class="section return"><dt>Returns</dt><dd>The handle of the MPI communicator *this wraps.</dd></dl>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">None</td><td>No throw guarantee. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="aadd9a3592211f25ca1c4ff08abfe0b88"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aadd9a3592211f25ca1c4ff08abfe0b88">&#9670;&nbsp;</a></span>gather() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void parallelzone::mpi_helpers::detail_::CommPPPIMPL::gather </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a6c066ef36dc8d95481e11b67d18fae6d">const_binary_reference</a>&#160;</td>
          <td class="paramname"><em>data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#aac4431ffb4dff217555cf3313b50106b">binary_reference</a>&#160;</td>
          <td class="paramname"><em>out_buffer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#af2364d851859f49c08b553ac68ea7aa2">opt_root_t</a>&#160;</td>
          <td class="paramname"><em>root</em> = <code>std::nullopt</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Binary-based gather call that can avoid copies. </p>
<p>This call is identical to the other overload of gather except that it does not allocate a buffer for the result. Instead the user must provide a pre-allocated buffer which is minimally of size <code>in_data.size() * (this-&gt;<a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#aeb6195c8087033113ce35d2eb9304597" title="Returns the number of ranks manged by comm()">size()</a>)</code>.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">data</td><td>The local bytes we are sending. The length of <code>data</code> must be the same on each process.</td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">out_buffer</td><td>A pre-allocated buffer to put the bytes into. If <code>root</code> is set <code>out_buffer</code> only needs to be allocated on the process with rank <code>root</code>, otherwise <code>out_buffer</code> must be allocated on every rank. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">root</td><td>The zero-based rank of the root process. This argument is optional. If it is set then only <code>root</code> will receive the result, if it is not set then every process gets a copy of the result. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a031d47e35fc8fa28bcb5795a161218b6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a031d47e35fc8fa28bcb5795a161218b6">&#9670;&nbsp;</a></span>gather() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a5dfa893cb94c40a93ad0f31b31169112">CommPPPIMPL::binary_gather_return</a> parallelzone::mpi_helpers::detail_::CommPPPIMPL::gather </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a6c066ef36dc8d95481e11b67d18fae6d">const_binary_reference</a>&#160;</td>
          <td class="paramname"><em>data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#af2364d851859f49c08b553ac68ea7aa2">opt_root_t</a>&#160;</td>
          <td class="paramname"><em>root</em> = <code>std::nullopt</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Binary-based gather call which creates a buffer for the result. </p>
<p>Assume that this communicator has <code>N</code> processes and that each process has a block of binary data that is <code>b</code> bytes long. For this call, the length of the binary data block must be <code>b</code> bytes on each process, but the values in the block can be (and typically are) different. To use this method, each process provides its local block of binary data and this method will create a single N * b element binary buffer such that elements <code>r * b</code> through <code>(r + 1) * b</code> are the <code>b</code> bytes from rank <code>r</code>. If <code>root</code> is set, then the output is only local to process <code>root</code>, otherwise each rank of the communicator will have a local copy of the output.</p>
<p>This is a zero-copy call; however, unless the output data can be directly used in binary format, it will be necessary to copy out of the resulting binary buffer to use the results. In some cases, such as when the object being sent can be interpreted as a contiguous binary buffer, it will be more efficient to use the other overload of gather, which allows you to directly gather the result into the target object. See the documentation for more details.</p>
<p>If <code>root</code> is set this method wraps a call to MPI_Gather, if <code>root</code> is not set then this call is analogous to MPI_Allgather. If you can not guarantee that each process is going to send the same number of bytes consider using <code>gatherv</code>.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">data</td><td>The local bytes to send. All processes must send the same number of bytes.</td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">root</td><td>The zero-based rank of the process which will get the data. If <code>root</code> is set this operation will wrap a call to MPI_Gather, otherwise (the default scenario), the call will be to MPI_Allgather.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A std::optional. If <code>root</code> is not set, then the std::optional will contain the gathered bytes on each rank of this communicator. If <code>root</code> is set then the resulting std::optional will only contain the bytes on the process with rank <code>root</code>. </dd></dl>

</div>
</div>
<a id="a18f149e5acccd0d544ddd6257c6a9170"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a18f149e5acccd0d544ddd6257c6a9170">&#9670;&nbsp;</a></span>gatherv()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a99f472c9d63bb84fb2c21d7df17164b4">CommPPPIMPL::binary_gatherv_return</a> parallelzone::mpi_helpers::detail_::CommPPPIMPL::gatherv </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a6c066ef36dc8d95481e11b67d18fae6d">const_binary_reference</a>&#160;</td>
          <td class="paramname"><em>data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#af2364d851859f49c08b553ac68ea7aa2">opt_root_t</a>&#160;</td>
          <td class="paramname"><em>root</em> = <code>std::nullopt</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td> const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Analog of gather(data, root) where the length of data can vary. </p>
<p>This class's two gather methods must send the same number of bytes from each process. This method relaxes that restriction. This method will allocate a buffer for the resulting bytes.</p>
<p>If <code>root</code> is not set this method wraps a call to MPI_Allgatherv. If <code>root</code> is set then this wraps a call to MPI_Gatherv.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">data</td><td>The local bytes we are sending. The length and content can vary from process to process. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">root</td><td>The zero-based rank of the process who should get the result. If <code>root</code> is set only the process with rank <code>root</code> will get the result, otherwise each process gets the result.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A std::optional around a pair. The first element of the pair is the concatenated bytes. The second element is an array such that the <code>i</code>-th element is how many bytes process <code>i</code> sent. The optional has a value on each process if <code>root</code> was not set and only on the process of rank <code>root</code> if <code>root</code> was set. </dd></dl>

</div>
</div>
<a id="a10dff15a59650bbd04067d1bddd57929"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a10dff15a59650bbd04067d1bddd57929">&#9670;&nbsp;</a></span>me()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a1126b67f0c397db003259bc0006f494e">size_type</a> parallelzone::mpi_helpers::detail_::CommPPPIMPL::me </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the rank of the current process. </p>
<p>In MPI processes are grouped into communicators. Each process in the communicator has a unique integral ID ranging from 0 to the size of the communicator. This method is used to retrieve the rank of the current process.</p>
<p>Calling this method is logically the same as calling MPI_Comm_rank on the result of <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a338e9de5958ab34e16999b9bd966cf32" title="Returns the MPI communicator behind *this.">comm()</a>. The actual call to MPI_Comm_rank is only done during construction and the result is cached in *this.</p>
<dl class="section return"><dt>Returns</dt><dd>The ID of the current process according to the communicator wrapped by *this.</dd></dl>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">None</td><td>No throw guarantee. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ac75b2fe878e602110638bf5d73b0b5ca"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac75b2fe878e602110638bf5d73b0b5ca">&#9670;&nbsp;</a></span>operator==()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">bool parallelzone::mpi_helpers::detail_::CommPPPIMPL::operator== </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html">CommPPPIMPL</a> &amp;&#160;</td>
          <td class="paramname"><em>rhs</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Compares *this to another PIMPL instance. </p>
<p>Two <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html" title="Basic implementations and state for the CommPP class.">CommPPPIMPL</a> instances are equal if they both wrap the same MPI communicator. Wrapping the same communicator is sufficient to guarantee that the communicators also have the same size and that the current process has the same rank.</p>
<p>The actual handles are compared with MPI_Comm_compare.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">rhs</td><td>The PIMPL to compare to *this.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>True if *this compares equal to <code>rhs</code> and false otherwise.</dd></dl>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">None</td><td>No throw guarantee. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="aeb6195c8087033113ce35d2eb9304597"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeb6195c8087033113ce35d2eb9304597">&#9670;&nbsp;</a></span>size()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a1126b67f0c397db003259bc0006f494e">size_type</a> parallelzone::mpi_helpers::detail_::CommPPPIMPL::size </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">noexcept</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the number of ranks manged by <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a338e9de5958ab34e16999b9bd966cf32" title="Returns the MPI communicator behind *this.">comm()</a> </p>
<p>In MPI, processes are grouped into communicators. This method is used to determine how many processes are associated with the communicator held by *this.</p>
<p>Calling this method is logically the same as calling MPI_Comm_size on the result of <a class="el" href="classparallelzone_1_1mpi__helpers_1_1detail___1_1CommPPPIMPL.html#a338e9de5958ab34e16999b9bd966cf32" title="Returns the MPI communicator behind *this.">comm()</a>. The actual call to MPI_Comm_size is only done during construction and the result is cached in *this.</p>
<dl class="section return"><dt>Returns</dt><dd>The number of processes associated with the wrapped MPI communicator.</dd></dl>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">None</td><td>No throw guarantee. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>src/parallelzone/mpi_helpers/commpp/detail_/<a class="el" href="commpp__pimpl_8hpp_source.html">commpp_pimpl.hpp</a></li>
<li>src/parallelzone/mpi_helpers/commpp/detail_/commpp_pimpl.cpp</li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
